#!/usr/bin/env bash

# if block_size isn't specified, we get an error about num_samples=0 on small datasets

# python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/gold/epoch-3 --num_train_epochs=3 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/gold/plaintext.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints --block_size=512 --overwrite_output_dir
# python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/gold/epoch-10 --num_train_epochs=10 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/gold/plaintext.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints --block_size=512 --overwrite_output_dir
# python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/gold/epoch-50 --num_train_epochs=50 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/gold/plaintext.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints --block_size=512 --overwrite_output_dir
# python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/gold/epoch-200 --num_train_epochs=200 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/gold/plaintext.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints --block_size=512 --overwrite_output_dir

python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/oracle/epoch-3 --num_train_epochs=3 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/oracle/decoding.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints --block_size=256 --overwrite_output_dir
python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/oracle/epoch-10 --num_train_epochs=10 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/oracle/decoding.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints --block_size=256 --overwrite_output_dir
python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/oracle/epoch-50 --num_train_epochs=50 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/oracle/decoding.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints --block_size=256 --overwrite_output_dir
python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/oracle/epoch-200 --num_train_epochs=200 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/oracle/decoding.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints --block_size=256 --overwrite_output_dir

# python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/wilkinson/epoch-3 --num_train_epochs=3 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/wilkinson/wilkinson_letters_cleaned.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints --block_size=1024 --overwrite_output_dir
# python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/wilkinson/epoch-10 --num_train_epochs=10 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/wilkinson/wilkinson_letters_cleaned.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints --block_size=1024 --overwrite_output_dir
# python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/wilkinson/epoch-20 --num_train_epochs=20 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/wilkinson/wilkinson_letters_cleaned.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints --block_size=1024 --overwrite_output_dir

# python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/cofea/epoch-1 --num_train_epochs=1 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/cofea/from-website-uncased.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints --overwrite_output_dir
# python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/cofea/epoch-3 --num_train_epochs=3 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/cofea/from-website-uncased.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints --overwrite_output_dir
# python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/cofea/epoch-10 --num_train_epochs=10 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/cofea/from-website-uncased.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints  --overwrite_output_dir
# python3.7 ~/transformers/examples/run_language_modeling.py --output_dir=language-models/cofea/epoch-20 --num_train_epochs=20 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=language-models/cofea/from-website-uncased.txt --do_eval --eval_data_file=data/eval/plaintext.txt --evaluate_during_training --eval_all_checkpoints  --overwrite_output_dir


